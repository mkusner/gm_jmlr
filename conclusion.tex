%!TEX root=gm_jmlr.tex 

\label{sec:conclusion}
In this paper, we comprehensively demonstrate the usefulness of gradient boosted regression trees (GBRT) for feature selection applications. We derive a novel algorithm that simultaneously performs feature selection and classification for widely differing settings. First, we show that if feature extraction requires a cost (e.g., CPU-time, monetary, patient discomfort) we can directly incorporate this cost into the gradient boosted optimization procedure of GBRT. Second, if we have prior knowledge that features are structured (e.g., for bio-informatics) we can seamlessly encode it in a regularization term for learning each tree. The power of a unified boosting procedure for (cost-sensitive) feature selection and classification is clearly demonstrated on multiple budgeted and real-world genomic datasets. In nearly all settings, \name{}-BL and \name{}-FS outperform the state-of-the-art.

As more and more practitioners look to machine learning for (a) efficient data-driven alternatives to intricate heuristics and (b) interpretable classification algorithms, the problems of cost-sensitive learning and feature selection are of increasing significance. Alongside the gradient boosted models proposed here, there are many directions for future work. Adapting GBRT-based approaches to the setting of \emph{anytime learning} \cite{zilberstein1996using}, in which a fixed cost budget for classification is unknown prior to test-time, would be worth further investigating, alongside the work of \citet{grubbspeedboost}. As well, it may be fruitful to consider ways to encode probabilistic feature structure into these models. We believe the models proposed here are a strong contribution towards addressing the requirements of practitioners for machine learning.