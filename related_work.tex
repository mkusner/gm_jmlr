%!TEX root=gm_jmlr.tex

In this section, we present prior work from two domains: budgeted learning and feature selection. 

\subsection{Prior work in budgeted learning}
Previous work in budgeted learning appears in many different applications. The most prominent was proposed by \citet{viola2004robust}. They greedily trained a cascade of classifiers with Adaboost~\citep{schapire1999brief} for fast object recognition.  \citet{cambazoglu2010early} then introduced the cascade framework to the setting of web-search ranking. Their algorithm is based on regular stage-wise regression, and during testing, they remove unpromising instances early-on using early-exits. \citet{Lefakis2010} and \citet{dundar2007joint} further extended the cascade framework by proposing soft-cascades, which re-weight inputs based on their probability of passing all stages. Very different from \name{}-BL, they do not consider the feature extraction cost explicitly during training and their algorithms are restricted to binary classification problems. 

To consider feature extraction cost, \citet{GaoKoller11} proposed to dynamically extract features during test-time. \citet{raykar2010designing} learn cost-sensitive classifier cascades by explicitly considering feature cost. They group features by their costs and restrict classifiers at each stage to only use small  feature subsets. \citet{pujara2011using} suggested the use of sampling to derive a cascade of classifiers with increasing cost for email spam filtering.  Most recently, \citet{chen2011} introduced Cronus, which explicitly considers the feature extraction cost during training and constructs a cascade to encourage removal of unpromising data points early-on. While effective, these algorithms require significant time to train. In contrast, \name{}-BL is a natural variant of fast stage-wise regression, and can operate in both regression and multi-class classification scenarios. 

% We pursue a very different (orthogonal) approach and do not optimize the cascade stages globally. Instead, we strictly incorporate the feature cost into the weak learners. Moreover, as our algorithm is a variant of stage-wise regression, it can operate naturally in both regression and multi-class classification scenarios. (Simultaneous with this publication,~\citet{grubb2012} also proposed a complementary approach to incorporate feature cost into gradient boosting.)

\subsection{Prior work in feature selection}
One of the most widely used feature selection algorithms is Lasso~\citep{tibshirani1996regression}. It minimizes the squared loss with $l_1$ regularization on the coefficient vector, which encourages sparse solutions.  Although scalable to very large data sets, Lasso models only linear correlations between features and labels and cannot discover non-linear feature dependencies. 

\citet{peng2005feature} proposed the Minimum Redundancy Maximum Relevance (mRMR) algorithm, which select features according to their mutual information with instance labels. Their objective function also penalizes selecting redundant features. However, computing mutual information is intractable when the training size is large. \citet{yamada2012high} introduced HSIC Lasso. Their algorithm introduces non-linearity by using kernel functions and performs feature selection by enforcing an $l_1$-norm on the coefficients of kernel matrices. The algorithm requires constructing kernel matrices for all features, thus its time and memory complexity scale quadratically with the dataset size. Moreover, both algorithms separate feature selection and classification, and require additional time and computation for training classifiers using the selected features.

Several other works avoid expensive kernel computation while maintaining non-linearity.  Grafting~\citep{perkins2003grafting} combines $l_1$ and $l_0$ regularization with a non-linear classifier based on a non-convex variant of the multi-layer perceptron. Feature selection for ranking using boosted trees~\citep{pan2009feature} and selects the top features with the highest relative importance scores. \cite{tuv2009feature} and \cite{trevor2009elements} use random forests to select features based on their frequency of use in tree splits. These algorithms may be seen as complimentary to the models proposed in this work. 
%by ranking all features from the forest by their accumulated impurity improvement in all splits. \cite{tuv2009feature} introduces artificial features and masking scores to Random Forest feature selection to further remove irrelevant features. 
% Finally, while not a feature selection method, \cite{greedymiser} employ Gradient Boosted Trees to learn cascades of classifiers to reduce test-time cost by incorporating feature extraction budgets into the classifier optimization. 