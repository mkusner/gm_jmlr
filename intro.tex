%!TEX root=gm_jmlr.tex 

Gradient Boosted Regression Trees (GBRT) \citep{friedman2001greedy} are widely used in many real-world applications. GBRT non-linearly partitions the input space using decision trees, and thus is readily suited for learning tasks with unbalanced and complex data. For instance, in a recent web-search ranking competition, eight winners of two competition tracks all use ensemble based decision tree algorithms \citep{chapelle2011yahoo}. In the meantime, compared to other classifiers, its unique functional optimization procedure makes feature extraction very easy. It separates feature extraction and classifier optimization into two separate stages, and thus structured feature information can be seamlessly incorporated. Moreover, its evaluation time is linear w.r.t. the number of testing inputs, and this makes GBRT very scalable to large feature extraction problems.

Feature extraction plays a crucial role %recently been revisited 
in two popular learning scenarios: budgeted learning \citep{xu2014classifier} and feature selection \citep{guyon2003introduction}. In the budgeted learning setting, researchers focus on reducing the computational costs (and potentially other related costs) of classification during test-time. This test-time computational cost often consists of two components: (a) the running time of the algorithm and (b) the time required for extracting features used by the algorithm. In this work, we focus on the scenario where the feature extraction cost is dominant. Imagine introducing a new feature to an algorithm that is executed one million times per day which improves the accuracy by 1\%, but also increases the extraction time by 1$s$ per execution. %and this algorithm is executed a million times per day. 
This new feature would require allocating 5 days of additional CPU time every day. Such scenarios are commonplace in large-scale web-search ranking \citep{zheng2007general} and email spam filtering \citep{weinberger2009feature}. Therefore feature extraction must be strictly accounted for. 

In the feature selection setting, researchers are concerned with how a classifier can predict a label and the features it uses. As the size of data sets grow, % increasingly larger and larger, 
a good feature selection algorithm should not only be able to identify relevant features, but also be scalable to large amount of training inputs. Additionally, in various applications such as bio-informatics~\citep{saeys2007review}, information about inter-feature dependency should also be considered during feature selection. In both budgeted learning and feature selection, efficiently extracting features for a very large data set plays a key role in designing a learning algorithm.

In this paper, we introduce a new algorithm: \fullname{} (\name{}) that is designed for efficient feature extraction in a large scale setting. \name{} is based on Gradient Boosted Regression Trees (GBRT), and therefore inherits many desirable properties that are suitable for feature extraction. Firstly, \name{} retains the capabilities of coping with non-linear interactions between features and labels from GBRT, and thus can discover non-linear dependencies. Secondly, unlike other feature extraction algorithms used in budgeted learning or feature selection, which use computationally-expensive kernel methods~\citep{scholkopf2001learning} or complex cascade structures \citep{cambazoglu2010early,chen2011,Saberian2010,Lefakis2010}, \name{} maintains non-linearity very efficiently, and thus scales to large data sets %containing a large number of samples 
while it is simultaneously %and is 
very easy to implement and use. Thirdly, due to the unique two-step functional optimization procedure %two steps function space optimization 
of GBRT, \name{} can easily handle side information about structured feature dependency. This structured feature dependency is commonly seen in applications such as bio-informatics \citep{saeys2007review} %feature selection and budgeted learning for 
and computer vision \citep{felzenszwalb2010object}. Finally, \name{} unifies feature extraction and classification into one optimization problem, pushing the feature extraction into the classifier learning. 

Two earlier short papers already introduce \name{} in two different settings, Greedy Miser~\citep{greedymiser} for budgeted learning and Gradient Boosted Feature Selection (GBFS)~\citep{xu2014gradient} for feature selection. However, this manuscript provides significantly more insight, discussion, and experimental results than prior work. The paper is organized as follows. In Section \ref{sec:background} we first define the notation used in the paper and describe some background knowledge. We then investigate the unique properties of GBRT and give insights why its two-step optimization is useful for feature extraction in Section~\ref{sec:gbrt}. In Section~\ref{sec:greedymiser}, we introduce and define the test-time budgeted learning setting. We then construct an optimization problem for test-time budgeted learning and derive our algorithm for budgeted learning (\name{}-BL) by solving the optimization problem. In Section \ref{sec:gbfs} we connect budgeted learning and feature selection, and introduce a variant of \name{} tailored for efficient feature selection (\name{}-FS). In Section \ref{sec:variance}, we introduce a simple analysis method for bounding the prediction variance of \name{}. In Section \ref{sec:results} we demonstrate the performance of our algorithm on several real-world benchmark data sets for budgeted learning and feature selection, and also discuss using the prediction variance analysis as a convergence tool. Section \ref{sec:related} reviews the prior and related work that inspires our paper. Finally, in Section \ref{sec:conclusion}, we conclude and propose several potential future approaches.



